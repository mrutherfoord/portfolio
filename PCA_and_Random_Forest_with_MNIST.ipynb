{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "PCA and Random Forest with MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrutherfoord/portfolio/blob/master/PCA_and_Random_Forest_with_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-0MRnwOhbHo",
        "colab_type": "text"
      },
      "source": [
        "# PCA and Random Forest with MNIST\n",
        "\n",
        "For this project I will be using the MNIST dataset with Random Forests with and without Principle Component Analysis to see what happens to the accuracy when I reduce the dimensionality of the data. This data contains 70,000 instances of 28x28 pixels of hand-written numbers (0-9), and is divided into a training set of 42,000 instances and a test set of 28,000 instances. Each instance is comprised of 784 variables that each represent one pixel in the 28x28 image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvTo1X1XhbHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing relevant modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# model building modules\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from time import time\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw_v_tdvhbHw",
        "colab_type": "text"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "In part one, I created a random forest module with 10 estimators and submitted it to Kaggle.com for an accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRR4GYUphbHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing data and separating the training set into and X and y dataframes\n",
        "mnist = pd.read_csv('train.csv')\n",
        "X = mnist.drop(['label'], axis=1)\n",
        "y = mnist['label']\n",
        "\n",
        "# importing test data as dataframe\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haohP51phbH1",
        "colab_type": "code",
        "outputId": "3c91a3ec-4f49-48c5-b162-c0aa11900b5c",
        "colab": {}
      },
      "source": [
        "# combining data for part 2\n",
        "mnist_combined = X.append(test, ignore_index=True)\n",
        "mnist_combined.shape\n",
        "print('Shape of combined data: {}'.format(mnist_combined.shape))\n",
        "print('-' * 50)\n",
        "\n",
        "#converting dataframes to array\n",
        "X_train = X.values\n",
        "print('Shape of training data (explanatory variables): {}'.format(X_train.shape))\n",
        "print('-' * 50)\n",
        "\n",
        "y_train = y.values\n",
        "y_train.reshape(-1,1)\n",
        "print('Shape of training data (target): {}'.format(y_train.shape))\n",
        "print('-' * 50)\n",
        "\n",
        "X_test = test.values\n",
        "print('Shape of testing data: {}'.format(X_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of combined data: (70000, 784)\n",
            "--------------------------------------------------\n",
            "Shape of training data (explanatory variables): (42000, 784)\n",
            "--------------------------------------------------\n",
            "Shape of training data (target): (42000,)\n",
            "--------------------------------------------------\n",
            "Shape of testing data: (28000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvC54KHShbH5",
        "colab_type": "text"
      },
      "source": [
        "As stated before, I used 10 estimators for the Random Forest. I also set max_features to 'sqrt'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP4SCHPUhbH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=10, max_features='sqrt', random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2gW1yI4hbH9",
        "colab_type": "code",
        "outputId": "8db7d7fb-caac-4943-ca9d-52a6a482a4bf",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#fitting random forest model with timing\n",
        "rf_clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 5.79 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSe_AJWphbIA",
        "colab_type": "code",
        "outputId": "27d6a218-7362-475e-c1ed-2eb080367b24",
        "colab": {}
      },
      "source": [
        "# predictions for Random Forest\n",
        "rf_clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC4v8s_zhbID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving predictions as dataframe\n",
        "rf_predictions = pd.DataFrame(rf_clf.predict(test), columns=['Label'])\n",
        "rf_predictions.index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzKLfWrUhbIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exporting datafram to .csv for Kaggle submission \n",
        "rf_predictions.to_csv('predictions.csv', index_label='ImageID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8rfXWhmhbII",
        "colab_type": "text"
      },
      "source": [
        "# First Kaggle Submission\n",
        "\n",
        "Score: .93814\n",
        "User ID: mrutherfoord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TNZQZkvhbIJ",
        "colab_type": "text"
      },
      "source": [
        "For only 10 estimators, the model was fairly accurate, although there is room for improvement. It also took 5.79 seconds to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKBScAfshbIJ",
        "colab_type": "text"
      },
      "source": [
        "# Parts 2 and 3\n",
        "\n",
        "For part 2, I used PCA on the entire dataset, preserving 95% of the varaince, then fit a new random forest with the reduced dataset. I was led to believe that scaling was included in scikit-learn's PCA, but it seems to not be the case. I chose to use StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DJLlQh3hbIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5DEiBG4hbIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_combined_scaled = scaler.fit_transform(mnist_combined)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrvRrpZGhbIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate pca with 95% variance\n",
        "pca = PCA(n_components=.95, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rar2BEQzhbIV",
        "colab_type": "code",
        "outputId": "65c41243-bfd7-4b8a-9b0f-c178c009a46d",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# fitting pca to entire dataset\n",
        "pca.fit(mnist_combined_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 15.3 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=42,\n",
              "  svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggtLjQBNhbIX",
        "colab_type": "code",
        "outputId": "e48202c4-6e5e-411d-a188-d69ee6e6ca18",
        "colab": {}
      },
      "source": [
        "print('Number of components left: {}'.format(pca.n_components_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of components left: 332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHV8laQAhbIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mapping PCA to training and testing sets\n",
        "pca_X_train = pca.transform(X_train_scaled)\n",
        "pca_X_test = pca.transform(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMt_-6d_hbId",
        "colab_type": "code",
        "outputId": "e30f6e1f-29b4-4aa0-8fb0-2b3d42ccd781",
        "colab": {}
      },
      "source": [
        "# Checking shape of reduced data\n",
        "print('Shape of reduced dataset: {}'.format(pca_X_train.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of reduced dataset: (42000, 332)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQcSY2AXhbIf",
        "colab_type": "code",
        "outputId": "a772312c-3fe0-47b6-d9dc-eeaee271ce59",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Timing and fitting new random forest model with reduced data\n",
        "rf_clf.fit(pca_X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 21.2 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otl-xsEthbIj",
        "colab_type": "text"
      },
      "source": [
        "It is interesting to see that the timing took almost 4 times as long. Researching this on Stack Overflow, I found that this can happen with PCA and random forests for a few reasons. First, the data reduction is not as large as it looks since I am using square root for max_features. Sqrt(784) is 28, and the sqrt(334) is 18.27. Second, reducing the amount of features can make it more difficult for the algorithm to find the best splits, which may require more iterations, and thus increased time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AlOr_MchbIj",
        "colab_type": "code",
        "outputId": "3dfc8f63-6942-447e-fdaa-4f1951709d48",
        "colab": {}
      },
      "source": [
        "# New random forest predictions\n",
        "rf_clf.predict(pca_X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 4, ..., 3, 9, 2], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWFYNdJJhbIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating dataframe of predictions\n",
        "pca_rf_predictions = pd.DataFrame(rf_clf.predict(pca_X_test), columns=['Label'])\n",
        "pca_rf_predictions.index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiTSx9vqhbIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exporting .csv for Kaggle submission\n",
        "pca_rf_predictions.to_csv('pca_predictions.csv', index_label='ImageID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTtvP0EghbIq",
        "colab_type": "code",
        "outputId": "f15792d2-c0c3-4931-f9a9-2090449c5c8f",
        "colab": {}
      },
      "source": [
        "pca_rf_predictions.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label\n",
              "1      2\n",
              "2      0\n",
              "3      4\n",
              "4      2\n",
              "5      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6b3Q5RhbIt",
        "colab_type": "text"
      },
      "source": [
        "# Second Kaggle Submission\n",
        "\n",
        "Score: .87028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jcV6bOEhbIu",
        "colab_type": "text"
      },
      "source": [
        "The accuracy was affected by the feature reduction. This can happen because features with lower variance can still be highly correlated with the target values. For example, if I was building a model for detecting the flu and had body mass and body temperature as features, body mass would have more much more variability, but body temperature would be more correlated with the target. (Flu example found on StackExchange). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l7_u-RNhbIu",
        "colab_type": "text"
      },
      "source": [
        "# Part 4\n",
        "\n",
        "The problem with part 2 is that I ran the PCA on both the training and testing sets, when it should only be fitted to the training set. Failure to do so can lead to data leakage and an overly optimistic model. This is because the model has now 'seen' data outside the training set and now knows about data that it shouldn't. \n",
        "\n",
        "For this part, I instead only fit the PCA to the training data, then mapped it to the testing data. I still used the scaled data and also added 10 fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlVkX_S4hbIv",
        "colab_type": "code",
        "outputId": "9febdb4c-5798-4653-d6ab-4f67f27f6ee6",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pca_X_train2 = pca.fit_transform(X_train_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 9.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmtLYA9dhbIx",
        "colab_type": "code",
        "outputId": "31976cb4-296f-488d-b029-95a9f249c99c",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pca_X_test2 = pca.transform(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 725 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBgUPWb1hbIz",
        "colab_type": "code",
        "outputId": "25b9b70e-d350-4148-c1dd-fa3394487ff1",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "rf_clf.fit(pca_X_train2, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 18.4 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ZYEVoshbI1",
        "colab_type": "text"
      },
      "source": [
        "This model took slightly less time than the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b93NOkn5hbI3",
        "colab_type": "code",
        "outputId": "78ff6be4-da1f-4882-dad4-c6140954b3c6",
        "colab": {}
      },
      "source": [
        "cv_score = cross_val_score(rf_clf, pca_X_train2, y_train, cv=10)\n",
        "print('Cross Validated Score: {}'.format(cv_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validated Score: [0.86471707 0.86941009 0.86815802 0.86574625 0.87119048 0.87330317\n",
            " 0.86496785 0.87228973 0.87416587 0.87416587]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECuiTniuhbI5",
        "colab_type": "text"
      },
      "source": [
        "It doesn't appear that only using only the training set for PCA changes the accuracy much in this case, but using PCA on the entire set can create an overly-optimistic model that doesn't generalize well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwOTWMqPhbI5",
        "colab_type": "code",
        "outputId": "8e128c94-e919-4efb-9b46-c9297a39187b",
        "colab": {}
      },
      "source": [
        "rf_clf.predict(pca_X_test2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 2, ..., 3, 9, 2], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 392
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nk9lE-AhbI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_rf_predictions2 = pd.DataFrame(rf_clf.predict(pca_X_test2), columns=['Label'])\n",
        "pca_rf_predictions2.index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuXFvcIshbI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_rf_predictions2.to_csv('pca_predictions2.csv', index_label='ImageID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kShO5fcthbJA",
        "colab_type": "code",
        "outputId": "7883dde0-9571-4e56-cbff-c8906c232538",
        "colab": {}
      },
      "source": [
        "pca_rf_predictions2.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label\n",
              "1      2\n",
              "2      0\n",
              "3      2\n",
              "4      4\n",
              "5      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uacqjI3hbJC",
        "colab_type": "text"
      },
      "source": [
        "# Third Kaggle Submission\n",
        "\n",
        "Score .86314\n",
        "\n",
        "This model is less accurate than in part two, which is to be expected since PCA was only used on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIHZ9uxYhbJD",
        "colab_type": "text"
      },
      "source": [
        "To see how accuracy improved with more trees, I used a random forest with 100 estimators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DQHXovzhbJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate Classifier - 100 \n",
        "rf_clf_100 = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6LmIH4MhbJG",
        "colab_type": "code",
        "outputId": "715cb16c-f678-4d9b-b46a-8337eb1274c3",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "rf_clf_100.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 1min 3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Risj0TklhbJJ",
        "colab_type": "code",
        "outputId": "feb21b29-0f7a-412f-8b0e-6d86b660feee",
        "colab": {}
      },
      "source": [
        "rf_clf_100.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1jBW6WmhbJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_predictions_100 = pd.DataFrame(rf_clf_100.predict(test), columns=['Label'])\n",
        "rf_predictions_100.index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8xRlPNuhbJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_predictions_100.to_csv('predictions_100.csv', index_label='ImageID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvLO28LthbJR",
        "colab_type": "text"
      },
      "source": [
        "# Fourth Kaggle Score\n",
        "\n",
        "Score: .96585"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2FOY2YghbJR",
        "colab_type": "text"
      },
      "source": [
        "It's clear that more trees will continue to improve the accuracy of this model. Performing a gridsearch as well as potentially stacking with other algorithms could help improve this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jveXXfKhhbJS",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "It's possible that I would recommend PCA for certain algorithms, but I'm not sure I would recommend it be used in conjunction with random forests for this particular dataset. The performance time suffers as the random forest uses more iterations to find the best splits. Also, as mentioned earlier, removing variables with lower variability compresses the data, but pixels with low variability may be highly correlated with the target. Removing them can reduce prediction accuracy. PCA can be a good choice, but it depends on data, and I didn't find it to be helpful in this case. "
      ]
    }
  ]
}